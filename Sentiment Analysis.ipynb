{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae01c7a",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "#### By Rohit Suryavanshi\n",
    "Work Timeline doc: https://docs.google.com/document/d/1IXqmf_iipkzNDwZqWczPdeCnnNbQvESU1ktOREng1D0/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72136d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from numpy.random import *\n",
    "import pandas as pd\n",
    "#import math\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3edf589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('training.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a5f7b",
   "metadata": {},
   "source": [
    "### Labels:\n",
    "\n",
    "0.   sadness\n",
    "1.   joy\n",
    "2.   love\n",
    "3.   anger\n",
    "4.   fear\n",
    "5.   surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0064c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f8f41",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd88c3e6",
   "metadata": {},
   "source": [
    "## Remove Punctuations\n",
    "#### Ref: https://studymachinelearning.com/text-preprocessing-removal-of-punctuations/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "451df6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "regular_punct = list(string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    punct_list = regular_punct\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, ' ')\n",
    "    return text.strip()\n",
    "df['punc_text']=df['text'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b924eb9",
   "metadata": {},
   "source": [
    "## Tokenising Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5f3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenised']=df.apply(lambda row: nltk.word_tokenize(row['punc_text']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e742c0fa",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065169bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "# init lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c99b12d",
   "metadata": {},
   "source": [
    "### Position Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50c964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d366a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.apply(.....) ## to get whole lemmatized with pos_tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa1e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=df['tokenised']\n",
    "text=text.tolist()\n",
    "for i in range(len(text)):\n",
    "    #tokens =  text[i]   #a['tokenised'][i]\n",
    "    #tag=nltk.pos_tag(tokens)\n",
    "    #for j in range(len(tokens)):\n",
    "    text[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in text[i]]\n",
    "df['lemmatized']= pd.Series((i for i in text))\n",
    "#df['lemmatized']= pd.Series((i for i in text))    \n",
    "\n",
    "\n",
    "#df['lemmatized']=df['tokenised'].progress_apply(lambda word: lemmatizer.lemmatize(word=word,pos='v') ) \n",
    "#df['lemmatized'] = df['tokenised'].apply(lambda x: x= [word for word in x.split() word=lemmatizer.lemmatize(word=word,pos='v')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55164ea8",
   "metadata": {},
   "source": [
    "## Removal of Stopwords\n",
    "##### Ref: https://stackabuse.com/removing-stop-words-from-strings-in-python/#usingpythonsnltklibraryÂ¶\n",
    "##### Ref: https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f8618f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# #nltk.download('stopwords')\n",
    "# text=df['lemmatized']\n",
    "# text=text.tolist()\n",
    "# n=len(df['lemmatized'])\n",
    "# for i in range(n):\n",
    "#     text[i] = [word for word in df['lemmatized'][i] if not word in stopwords.words()]    \n",
    "# df['cleaned_text']= pd.Series((i for i in text))\n",
    "# df.head()\n",
    "# Use\n",
    "#df.apply[lambda x: ]\n",
    "\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "864c2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "df['string_text']= df['lemmatized'].str.join(\" \")\n",
    "df['cleaned_text'] = df['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93629c",
   "metadata": {},
   "source": [
    "###### Sample to use df.apply on dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1966a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop = stopwords.words('english')\n",
    "\n",
    "# pos_tweets = [('I love this car', 'positive'),\n",
    "#     ('This view is amazing', 'positive'),\n",
    "#     ('I feel great this morning', 'positive'),\n",
    "#     ('I am so excited about the concert', 'positive'),\n",
    "#     ('He is my best friend', 'positive')]\n",
    "\n",
    "# test = pd.DataFrame(pos_tweets)\n",
    "# test.columns = [\"tweet\",\"class\"]\n",
    "\n",
    "# # Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "# #test['tweet_without_stopwords'] = test['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# print(test['tweet'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e6fea",
   "metadata": {},
   "source": [
    "### New Dataset with Pre-Processed Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa66d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(df['cleaned_text'])\n",
    "new_df['label']=df['label']\n",
    "new_df.to_csv('pre-processed_data.csv',index=False)\n",
    "#len(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11df549",
   "metadata": {},
   "source": [
    "##### Sample to use classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef0b3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# X, y = load_iris(return_X_y=True)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# type(X_train)\n",
    "# gnb = GaussianNB()\n",
    "# y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "# print(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd482a",
   "metadata": {},
   "source": [
    "## Basic Way to Approach\n",
    "1. Splitting dataframe in 80-20 way for training and test(C.V.) datasets\n",
    "2. Import TF-IDF vectorizer and corpus from x_train and x_test\n",
    "3. Apply vectorizer on corpus and obtain array containing TF-IDF values for each word\n",
    "4. Apply Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7bab6fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting into training and test(C.V.) data set\n",
    "# sc=0\n",
    "# while sc<0.94:\n",
    "n=len(new_df['label'])\n",
    "idx_train = np.random.choice(n,int(n),replace=False) ## Gives us a list\n",
    "#idx_test = np.random.choice(n,int(n*0.2),replace=False) ## int bcuz n*0.2 is float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "61c83cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=new_df.loc[idx_train]\n",
    "x_train=df_train['cleaned_text']\n",
    "y_train=df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b77f57b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train,df_test=new_df.loc[idx_train],new_df.loc[idx_test]\n",
    "# x_train,x_test=df_train['cleaned_text'],df_test['cleaned_text']\n",
    "# y_train,y_test=df_train['label'],df_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "53e4ac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus_train=x_train.tolist()\n",
    "#corpus_test=x_test.tolist()\n",
    "\n",
    "vectorizer =  CountVectorizer() \n",
    "#vectorizer =  TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(corpus_train)\n",
    "#X_test = vectorizer.transform(corpus_test)\n",
    "#type(X_train)\n",
    "vocab=vectorizer.get_feature_names_out()\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d9905f",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Used Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e95ae678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9c92dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zoom difficulties feel like give everything feel helpless alone desert cast ways voice action others another story zoom also temporarily loose view full picture\n"
     ]
    }
   ],
   "source": [
    "## Max length(words) in a sentence\n",
    "print(max(corpus_train))\n",
    "max_length=24 ## counted by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de91e7e2",
   "metadata": {},
   "source": [
    "### Used One-hot Encoding and Padding to input it into Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "48a6bede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "encoded=[one_hot(i,vocab_size) for i in corpus_train]\n",
    "padded=pad_sequences(encoded,maxlen=max_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6d7ed1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=padded\n",
    "#x=encoded\n",
    "#length = max(map(len, x))\n",
    "#x=np.array([xi+[None]*(length-len(xi)) for xi in x])\n",
    "#X=np.asarray(x,dtype=object)\n",
    "#print(x)\n",
    "#print(type(x))\n",
    "y=y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de45ff2",
   "metadata": {},
   "source": [
    "### Developing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "91ef83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "\n",
    "embeded_vector_size=50\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\n",
    "#model.add(Flatten())\n",
    "#model.add(Conv1D(64,kernel_size=(50,50),activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(units=16,activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3b2b0cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 24, 50)            605350    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 8)                 408       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 6)                 54        \n",
      "=================================================================\n",
      "Total params: 605,812\n",
      "Trainable params: 605,812\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5c2e3e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 60s 4ms/step - loss: 0.9976 - accuracy: 0.6401\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 61s 4ms/step - loss: 0.4187 - accuracy: 0.8554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d21119c550>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a04d666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 648us/step - loss: 0.2914 - accuracy: 0.9144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9143750071525574"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss,accuracy=model.evaluate(x,y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "acd963f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv')\n",
    "## REMOVE PUNCTUATION\n",
    "df_test['text']=df_test['text'].apply(remove_punctuation)\n",
    "## TOKENISATION\n",
    "df_test.text=df_test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "## LEMMATIZATION\n",
    "submit=df_test['text']\n",
    "submit=submit.tolist()\n",
    "for i in range(len(submit)):\n",
    "    submit[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in submit[i]]\n",
    "## REMOVE STOPWORDS\n",
    "submit=pd.Series(submit)\n",
    "df_test['string_text']= submit.str.join(\" \")\n",
    "df_test['cleaned_text'] = df_test['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "submit=df_test['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "dbd37047",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_submit=submit.tolist()\n",
    " \n",
    "vectorizer =  CountVectorizer() \n",
    "\n",
    "submit_hmm = vectorizer.fit_transform(corpus_submit)\n",
    "#X_test = vectorizer.transform(corpus_test)\n",
    "#type(X_train)\n",
    "vocab=vectorizer.get_feature_names_out(submit_hmm)\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "9a3c5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(max(corpus_submit))\n",
    "max_length=24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7e5acbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_submit=[one_hot(i,vocab_size) for i in corpus_submit]\n",
    "submit=pad_sequences(encoded_submit,maxlen=max_length,padding='post')\n",
    "#submit=encoded_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0dd7c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=model.predict(submit)\n",
    "#print(p[:7])\n",
    "# p[np.where(a==np.max(p))] = 1\n",
    "b = np.zeros_like(p)\n",
    "b[np.arange(len(p)), p.argmax(1)] = 1\n",
    "kaggle=b\n",
    "# kaggle=np.round(p)\n",
    "# kaggle = p\n",
    "#print(kaggle[0:7])\n",
    "def labelling(matrix):\n",
    "    n=len(matrix)\n",
    "    lst=[1]*n\n",
    "    for i in range(n):\n",
    "        arr=matrix[i]\n",
    "        arr=list(arr)\n",
    "        idx=arr.index(max(arr))\n",
    "        lst[i]=idx        \n",
    "    return lst\n",
    "a=labelling(kaggle)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5b138f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle=pd.DataFrame(columns=['id','label'])\n",
    "n=len(a)\n",
    "k=np.arange(n)+1\n",
    "kaggle['id']=pd.Series(k)\n",
    "kaggle['label']=pd.Series(a)\n",
    "len(kaggle['id'])\n",
    "kaggle.to_csv('submission_tensor.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426cdd4",
   "metadata": {},
   "source": [
    "# Naive Bayes Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3413a3",
   "metadata": {},
   "source": [
    "#### Implemented Scaling - Decreased Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34bf6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# scaler1 = preprocessing.StandardScaler(with_mean=False).fit(X_train) ## in [-2,2]\n",
    "# scaler2 = preprocessing.StandardScaler(with_mean=False).fit(X_test) ## with_mean=False is used for sparse matrices\n",
    "# X_train_scaled = scaler1.transform(X_train)\n",
    "# X_test_scaled = scaler2.transform(X_test)\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler() # to fit array in [0,1] range\n",
    "\n",
    "# min_max_scaler = preprocessing.MaxAbsScaler()\n",
    "# # This estimator scales and translates each feature individually such that the maximal absolute value of each feature \n",
    "# # in the training set will be 1.0. \n",
    "# # It does not shift/center the data, and thus does not destroy any sparsity.\n",
    "# # This scaler can also be applied to sparse CSR or CSC matrices.\n",
    "\n",
    "# X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "# X_test_scaled = min_max_scaler.fit_transform(X_test)\n",
    "\n",
    "# X_train_scaled.array\n",
    "# X_test_scaled.toarray()\n",
    "# y_train.array\n",
    "# y_test.array\n",
    "\n",
    "X_train_scaled=X_train\n",
    "#X_test_scaled=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b11c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB ## something new score - 0.7\n",
    "from sklearn.naive_bayes import CategoricalNB ##sparse error \n",
    "from sklearn.naive_bayes import GaussianNB ##sparse error\n",
    "from sklearn.naive_bayes import ComplementNB ## dimension mismatch - max_score - 0.933125\n",
    "from sklearn.naive_bayes import MultinomialNB ## dimension mismatch - 0.8\n",
    "\n",
    "model = ComplementNB() ## 0.7240"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79625339",
   "metadata": {},
   "source": [
    "### ENSEMBLE LEARNING\n",
    "##### Decreased Accuracy since Complement NB was way stronger than multi and Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENSEMBLE LEARNING\n",
    "\n",
    "# model2 = MultinomialNB() ## 0.61\n",
    "# model3 = BernoulliNB() ## 0.61400\n",
    "# Cnb=model1.fit(X_train_scaled, y_train)\n",
    "# Mnb=model2.fit(X_train_scaled, y_train)\n",
    "# Bnb=model3.fit(X_train_scaled, y_train)\n",
    "\n",
    "# from sklearn.ensemble import VotingClassifier\n",
    "# #create a dictionary of our models\n",
    "# estimators=[('Complement', Cnb), ('Multinomial', Mnb), ('Bernoulli', Bnb)]\n",
    "# #create our voting classifier, inputting our models\n",
    "# ensemble = VotingClassifier(estimators, voting='hard')\n",
    "# submission=ensemble.fit(X_train_scaled, y_train)\n",
    "# ensemble.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d20eec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=model.fit(X_train_scaled, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60ac767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('test.csv')\n",
    "## REMOVE PUNCTUATION\n",
    "df_test['text']=df_test['text'].apply(remove_punctuation)\n",
    "## TOKENISATION\n",
    "df_test.text=df_test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
    "## LEMMATIZATION\n",
    "submit=df_test['text']\n",
    "submit=submit.tolist()\n",
    "for i in range(len(submit)):\n",
    "    submit[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in submit[i]]\n",
    "## REMOVE STOPWORDS\n",
    "submit=pd.Series(submit)\n",
    "df_test['string_text']= submit.str.join(\" \")\n",
    "df_test['cleaned_text'] = df_test['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "submit=df_test['cleaned_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dae461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=len(submit)\n",
    "corpus_submit=submit.tolist()\n",
    "submit = vectorizer.transform(corpus_submit)\n",
    "#submit_scaled = min_max_scaler.fit_transform(submit)\n",
    "submit=submission.predict(submit)\n",
    "kaggle=pd.DataFrame(columns=['id','label'])\n",
    "k=np.arange(n)+1\n",
    "kaggle['id']=pd.Series(k)\n",
    "kaggle['label']=pd.Series(submit)\n",
    "len(kaggle['id'])\n",
    "kaggle.to_csv('submission_corrected.csv',index=False)\n",
    "\n",
    "## WITHOUT SCALING - 0.73400, IMPROVES BY 0.1\n",
    "## WITH SCALING, DOWN BY 0.01 FOR MAX ABS \n",
    "##               DOWN BY 0.12 FOR STANDARD SCALER  \n",
    "## WITHOUT STOPWORDS - 0.654\n",
    "## USING COUNT_VECTORIZER - 0.674\n",
    "## with full dataset as train - 0.728 and score - 0.93\n",
    "## BY REPETITION, 0.74200\n",
    "## Spell checker and contractions brings down score\n",
    "## CURRENT - FULL SET AND ADDED PUNCTUATIONS - increase by 0.004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0a79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6b2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a510e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8203644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea42299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024249a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a1ef77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74e78bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]], shape=(2, 3), dtype=float32)\n",
      "(2, 3)\n",
      "<dtype: 'float32'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[14., 32.],\n",
       "       [32., 77.]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "x @ tf.transpose(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a506c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=pd.read_csv('play_data.csv')\n",
    "y=tdf.label.values\n",
    "x=np.column_stack((tdf.YearsExperience.values,tdf.Salary.values))\n",
    "np.random.shuffle(x)  \n",
    "#type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c42e03a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 2s 58ms/step - loss: 0.6931 - accuracy: 0.4667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1de25548490>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.choice()\n",
    "model=keras.Sequential([keras.layers.Dense(16,input_shape=(2,),activation='relu'),keras.layers.Dense(2,activation='relu')])\n",
    "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "model.fit(x,y,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497ca7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6b6e1233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 12107)\n"
     ]
    }
   ],
   "source": [
    "x=X_train.toarray()\n",
    "y=y_train.values\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f220c38",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4572/752717392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSparseCategoricalCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m                **kwargs):\n\u001b[0;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m-> 1430\u001b[1;33m   return convert_to_tensor_v2(\n\u001b[0m\u001b[0;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   \u001b[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m   return convert_to_tensor(\n\u001b[0m\u001b[0;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m   \u001b[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ML\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "model=keras.Sequential([keras.layers.Dense(16,input_shape=(12107,),activation='relu'),keras.layers.Dense(6,activation='relu')])\n",
    "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "y=new_df.label.values\n",
    "model.fit(x[0:1000],y[0:1000],batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b3d655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81220d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de5e536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425b611",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbed85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6cdd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b2674b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d883048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c209e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0740122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76796f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc7b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0c4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9e6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57325a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010e4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c641895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6046b547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
