{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitsurya26/WiDS-/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae01c7a",
      "metadata": {
        "id": "dae01c7a"
      },
      "source": [
        "# Sentiment_Analysis\n",
        "#### By Rohit Suryavanshi\n",
        "Work Timeline doc: https://docs.google.com/document/d/1IXqmf_iipkzNDwZqWczPdeCnnNbQvESU1ktOREng1D0/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b72136d2",
      "metadata": {
        "scrolled": true,
        "id": "b72136d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbb4df3c-4d42-40cc-8489-4f334a7b9c21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3edf589c",
      "metadata": {
        "id": "3edf589c"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/rohitsurya26/WiDS-/main/training.csv?token=GHSAT0AAAAAABQZ6QUIRAC5LYR25NLDXITWYPODPZQ'\n",
        "df = pd.read_csv(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635a5f7b",
      "metadata": {
        "id": "635a5f7b"
      },
      "source": [
        "### Labels:\n",
        "\n",
        "0.   sadness\n",
        "1.   joy\n",
        "2.   love\n",
        "3.   anger\n",
        "4.   fear\n",
        "5.   surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0064c08",
      "metadata": {
        "id": "f0064c08"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0f8f41",
      "metadata": {
        "id": "2f0f8f41"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd88c3e6",
      "metadata": {
        "id": "dd88c3e6"
      },
      "source": [
        "## Remove Punctuations\n",
        "#### Ref: https://studymachinelearning.com/text-preprocessing-removal-of-punctuations/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "451df6ba",
      "metadata": {
        "id": "451df6ba"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "regular_punct = list(string.punctuation)\n",
        "def remove_punctuation(text):\n",
        "    punct_list = regular_punct\n",
        "    for punc in punct_list:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, ' ')\n",
        "    return text.strip()\n",
        "df['punc_text']=df['text'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b924eb9",
      "metadata": {
        "id": "5b924eb9"
      },
      "source": [
        "## Tokenising Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5f3fbf",
      "metadata": {
        "id": "7a5f3fbf"
      },
      "outputs": [],
      "source": [
        "#nltk.download('punkt')\n",
        "df['tokenised']=df.apply(lambda row: nltk.word_tokenize(row['punc_text']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e742c0fa",
      "metadata": {
        "id": "e742c0fa"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065169bc",
      "metadata": {
        "id": "065169bc"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa1e5df",
      "metadata": {
        "id": "5fa1e5df"
      },
      "outputs": [],
      "source": [
        "text=df['tokenised']\n",
        "text=text.tolist()\n",
        "for i in range(len(text)):\n",
        "    text[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in text[i]]\n",
        "df['lemmatized']= pd.Series((i for i in text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55164ea8",
      "metadata": {
        "id": "55164ea8"
      },
      "source": [
        "## Removal of Stopwords\n",
        "##### Ref: https://stackabuse.com/removing-stop-words-from-strings-in-python/#usingpythonsnltklibraryÂ¶\n",
        "##### Ref: https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864c2170",
      "metadata": {
        "id": "864c2170"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "df['string_text']= df['lemmatized'].str.join(\" \")\n",
        "df['cleaned_text'] = df['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07e6fea",
      "metadata": {
        "id": "f07e6fea"
      },
      "source": [
        "### New Dataset with Pre-Processed Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa66d5dc",
      "metadata": {
        "id": "fa66d5dc"
      },
      "outputs": [],
      "source": [
        "new_df = pd.DataFrame(df['cleaned_text'])\n",
        "new_df['label']=df['label']\n",
        "new_df.to_csv('pre-processed_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "TvXGxSlMHhKM"
      },
      "id": "TvXGxSlMHhKM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bab6fdc",
      "metadata": {
        "id": "7bab6fdc"
      },
      "outputs": [],
      "source": [
        "## Splitting into training and test(C.V.) data set\n",
        "n=len(new_df['label'])\n",
        "idx_train = np.random.choice(n,int(n),replace=False) ## Gives us a list\n",
        "#idx_test = np.random.choice(n,int(n*0.2),replace=False) ## int bcuz n*0.2 is float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c83cca",
      "metadata": {
        "id": "61c83cca"
      },
      "outputs": [],
      "source": [
        "df_train=new_df.loc[idx_train]\n",
        "x_train=df_train['cleaned_text']\n",
        "y_train=df_train['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77f57b5",
      "metadata": {
        "id": "b77f57b5"
      },
      "outputs": [],
      "source": [
        "# df_train,df_test=new_df.loc[idx_train],new_df.loc[idx_test]\n",
        "# x_train,x_test=df_train['cleaned_text'],df_test['cleaned_text']\n",
        "# y_train,y_test=df_train['label'],df_test['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e4ac3a",
      "metadata": {
        "id": "53e4ac3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus_train=x_train.tolist()\n",
        "\n",
        "vectorizer =  CountVectorizer() \n",
        "#vectorizer =  TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(corpus_train)\n",
        "\n",
        "vocab=vectorizer.get_feature_names_out()\n",
        "vocab_size=len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tagging(corpus,vocab):\n",
        "    keys=list(range(len(vocab)))\n",
        "    keys=[x+1 for x in keys]\n",
        "    text_dict = dict(zip(vocab,keys))\n",
        "    n=len(corpus)\n",
        "    lst=[1]*n\n",
        "    for i in range(n):\n",
        "        string=corpus[i].split()\n",
        "        lst[i]=[text_dict.get(j) for j in string]\n",
        "    return lst\n",
        "def padding(x,n_max):\n",
        "    new_x=x\n",
        "    n=len(x)\n",
        "    for i in range(n):\n",
        "        new_x[i] = x[i] + [0] * (n_max - len(x[i]))\n",
        "    return new_x    \n",
        "\n",
        "n_max=40 ## for corpus_train    \n",
        "\n",
        "x_encoded=tagging(corpus_train,vocab)   \n",
        "x_padded=padding(x_encoded,n_max)\n",
        "\n",
        "x_train=np.array(x_padded,dtype='float32')\n",
        "idx_NaNs = np.isnan(x_train)\n",
        "x_train[idx_NaNs]=0\n",
        "# lst_nan=np.where(np.isnan(x_train).any(axis=1)) # rows where any value is nan\n",
        "# x_train=np.delete(x_train,lst_nan,axis=0)\n",
        "# y_train=np.delete(y_train,lst_nan,axis=0)\n"
      ],
      "metadata": {
        "id": "leMYr31G7wsY"
      },
      "id": "leMYr31G7wsY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[1, 2, 3], [0, 3, float('nan')]])\n",
        "where_are_NaNs = np.isnan(a)\n",
        "a[where_are_NaNs] = 0\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6R-tRrhaWRr",
        "outputId": "62c78054-18a4-49aa-8290-100e8f8d9dd3"
      },
      "id": "B6R-tRrhaWRr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1. 2. 3.]\n",
            " [0. 3. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://raw.githubusercontent.com/rohitsurya26/WiDS-/main/test.csv?token=GHSAT0AAAAAABQZ6QUJ4FG7Y2PHQUCDDDYOYPODQJA'\n",
        "df_test=pd.read_csv(url)"
      ],
      "metadata": {
        "id": "cLr4_9IMzGFr"
      },
      "id": "cLr4_9IMzGFr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## REMOVE PUNCTUATION\n",
        "df_test['text']=df_test['text'].apply(remove_punctuation)\n",
        "## TOKENISATION\n",
        "df_test.text=df_test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
        "## LEMMATIZATION\n",
        "submit=df_test['text']\n",
        "submit=submit.tolist()\n",
        "for i in range(len(submit)):\n",
        "    submit[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in submit[i]]\n",
        "## REMOVE STOPWORDS\n",
        "submit=pd.Series(submit)\n",
        "df_test['string_text']= submit.str.join(\" \")\n",
        "df_test['cleaned_text'] = df_test['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "submit=df_test['cleaned_text']"
      ],
      "metadata": {
        "id": "xrBUQpqOy9Z_"
      },
      "id": "xrBUQpqOy9Z_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_submit=submit.tolist()"
      ],
      "metadata": {
        "id": "B1sgCJWsOZcp"
      },
      "id": "B1sgCJWsOZcp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submit_encoded=tagging(corpus_submit,vocab)\n",
        "submit_padded=padding(submit_encoded,n_max)    \n",
        "x_test=np.array(submit_padded)\n",
        "x_test=np.array(x_test,dtype='float32')\n",
        "idx_NaNs = np.isnan(x_test)\n",
        "x_test[idx_NaNs]=0"
      ],
      "metadata": {
        "id": "ln3HcYPQ0CCL"
      },
      "id": "ln3HcYPQ0CCL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "90d9905f",
      "metadata": {
        "id": "90d9905f"
      },
      "source": [
        "# Deep Learning\n",
        "### Used Tensorflow and Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e95ae678",
      "metadata": {
        "id": "e95ae678"
      },
      "outputs": [],
      "source": [
        "import tensorflow \n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de91e7e2",
      "metadata": {
        "id": "de91e7e2"
      },
      "source": [
        "### Used One-hot Encoding and Padding to input it into Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a6bede",
      "metadata": {
        "id": "48a6bede"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras.preprocessing.text import one_hot\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# encoded=[one_hot(i,vocab_size) for i in corpus_train]\n",
        "# padded=pad_sequences(encoded,maxlen=max_length,padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7ed1f2",
      "metadata": {
        "id": "6d7ed1f2"
      },
      "outputs": [],
      "source": [
        "# x_train=np.array(x_train,dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lst_nan=np.where(np.isnan(x_train).any(axis=1)) # rows where any value is nan\n",
        "# x_train=np.delete(x_train,lst_nan,axis=0)\n",
        "# y_train=np.delete(y_train,lst_nan,axis=0)"
      ],
      "metadata": {
        "id": "3yvn1wJ8smYr"
      },
      "id": "3yvn1wJ8smYr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4de45ff2",
      "metadata": {
        "id": "4de45ff2"
      },
      "source": [
        "### Developing Model and Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ef83a2",
      "metadata": {
        "id": "91ef83a2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "\n",
        "embed_size=45\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size+1,output_dim=embed_size,mask_zero=True,input_length=n_max))\n",
        "# model.add(Flatten())\n",
        "model.add(Conv1D(32,15,activation='relu'))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(units=64,activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2b0cb2",
      "metadata": {
        "id": "3b2b0cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9714861-8d9a-4926-fd48-90b9d6dd3f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_39\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_39 (Embedding)    (None, 40, 45)            544860    \n",
            "                                                                 \n",
            " conv1d_28 (Conv1D)          (None, 26, 32)            21632     \n",
            "                                                                 \n",
            " global_average_pooling1d_34  (None, 32)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 64)                2112      \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 6)                 390       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 568,994\n",
            "Trainable params: 568,994\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2e3e54",
      "metadata": {
        "id": "5c2e3e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5ef05b0-267b-49da-f3dc-7e759aa1b57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "16000/16000 [==============================] - 136s 8ms/step - loss: 0.9107 - accuracy: 0.6094\n",
            "Epoch 2/5\n",
            "16000/16000 [==============================] - 134s 8ms/step - loss: 0.3347 - accuracy: 0.8836\n",
            "Epoch 3/5\n",
            "16000/16000 [==============================] - 133s 8ms/step - loss: 0.1945 - accuracy: 0.9316\n",
            "Epoch 4/5\n",
            "16000/16000 [==============================] - 133s 8ms/step - loss: 0.1263 - accuracy: 0.9546\n",
            "Epoch 5/5\n",
            "16000/16000 [==============================] - 130s 8ms/step - loss: 0.0851 - accuracy: 0.9693\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feed01ea710>"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ],
      "source": [
        "model.fit(x_train, y_train, epochs=5, batch_size=1,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04d666a",
      "metadata": {
        "id": "a04d666a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "599932d3-4c64-4d6b-b71b-32c36172c888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500/500 [==============================] - 2s 3ms/step - loss: 0.0431 - accuracy: 0.9845\n",
            "Loss: 0.04306742548942566\n",
            "Accuracy: 0.984499990940094\n"
          ]
        }
      ],
      "source": [
        "loss,accuracy=model.evaluate(x_train,y_train)\n",
        "print('Loss:',loss)\n",
        "print('Accuracy:',accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction"
      ],
      "metadata": {
        "id": "z71tJuA5jiSt"
      },
      "id": "z71tJuA5jiSt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd7c180",
      "metadata": {
        "id": "0dd7c180"
      },
      "outputs": [],
      "source": [
        "p=model.predict(x_test)\n",
        "b = np.zeros_like(p)\n",
        "b[np.arange(len(p)), p.argmax(1)] = 1\n",
        "kaggle=b\n",
        "def labelling(matrix):\n",
        "    n=len(matrix)\n",
        "    lst=[1]*n\n",
        "    for i in range(n):\n",
        "        arr=matrix[i]\n",
        "        arr=list(arr)\n",
        "        idx=arr.index(max(arr))\n",
        "        lst[i]=idx        \n",
        "    return lst\n",
        "a=labelling(kaggle)        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV Submission File"
      ],
      "metadata": {
        "id": "ulcJ9HFWjs5N"
      },
      "id": "ulcJ9HFWjs5N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b138f27",
      "metadata": {
        "id": "5b138f27"
      },
      "outputs": [],
      "source": [
        "kaggle=pd.DataFrame(columns=['id','label'])\n",
        "n=len(a)\n",
        "k=np.arange(n)+1\n",
        "kaggle['id']=pd.Series(k)\n",
        "kaggle['label']=pd.Series(a)\n",
        "# print(len(kaggle['id']))\n",
        "kaggle.to_csv('submission_colab.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4426cdd4",
      "metadata": {
        "id": "4426cdd4"
      },
      "source": [
        "# Naive Bayes Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35bd482a",
      "metadata": {
        "id": "35bd482a"
      },
      "source": [
        "## Basic Way to Approach\n",
        "1. Splitting dataframe in 80-20 way for training and test(C.V.) datasets\n",
        "2. Import TF-IDF vectorizer and corpus from x_train and x_test\n",
        "3. Apply vectorizer on corpus and obtain array containing TF-IDF values for each word\n",
        "4. Apply Complement Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(new_df['label'])\n",
        "idx_train = np.random.choice(n,int(n),replace=False)"
      ],
      "metadata": {
        "id": "5HGVCM-7OXgA"
      },
      "id": "5HGVCM-7OXgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=new_df.loc[idx_train]\n",
        "x_train=df_train['cleaned_text']\n",
        "y_train=df_train['label']"
      ],
      "metadata": {
        "id": "iXzVOAj5OcRp"
      },
      "id": "iXzVOAj5OcRp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus_train=x_train.tolist()\n",
        "\n",
        "vectorizer =  CountVectorizer() \n",
        "#vectorizer =  TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(corpus_train)"
      ],
      "metadata": {
        "id": "fpxk6Mp3OM59"
      },
      "id": "fpxk6Mp3OM59",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8d3413a3",
      "metadata": {
        "id": "8d3413a3"
      },
      "source": [
        "#### Implemented Scaling - Decreased Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bf6bdf",
      "metadata": {
        "id": "34bf6bdf"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "# scaler1 = preprocessing.StandardScaler(with_mean=False).fit(X_train) ## in [-2,2]\n",
        "# scaler2 = preprocessing.StandardScaler(with_mean=False).fit(X_test) ## with_mean=False is used for sparse matrices\n",
        "# X_train_scaled = scaler1.transform(X_train)\n",
        "# X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "# min_max_scaler = preprocessing.MinMaxScaler() # to fit array in [0,1] range\n",
        "\n",
        "# min_max_scaler = preprocessing.MaxAbsScaler()\n",
        "# # This estimator scales and translates each feature individually such that the maximal absolute value of each feature \n",
        "# # in the training set will be 1.0. \n",
        "# # It does not shift/center the data, and thus does not destroy any sparsity.\n",
        "# # This scaler can also be applied to sparse CSR or CSC matrices.\n",
        "\n",
        "# X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
        "# X_test_scaled = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "# X_train_scaled.array\n",
        "# X_test_scaled.toarray()\n",
        "# y_train.array\n",
        "# y_test.array\n",
        "\n",
        "X_train_scaled=X_train\n",
        "#X_test_scaled=X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b11c21",
      "metadata": {
        "id": "21b11c21"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB ## something new score - 0.7\n",
        "from sklearn.naive_bayes import CategoricalNB ##sparse error \n",
        "from sklearn.naive_bayes import GaussianNB ##sparse error\n",
        "from sklearn.naive_bayes import ComplementNB ## dimension mismatch - max_score - 0.933125\n",
        "from sklearn.naive_bayes import MultinomialNB ## dimension mismatch - 0.8\n",
        "\n",
        "model = ComplementNB() ## 0.7240"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79625339",
      "metadata": {
        "id": "79625339"
      },
      "source": [
        "### ENSEMBLE LEARNING\n",
        "##### Decreased Accuracy since Complement NB was way stronger than multi and Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fc7f2d",
      "metadata": {
        "id": "51fc7f2d"
      },
      "outputs": [],
      "source": [
        "### ENSEMBLE LEARNING\n",
        "\n",
        "# model2 = MultinomialNB() ## 0.61\n",
        "# model3 = BernoulliNB() ## 0.61400\n",
        "# Cnb=model1.fit(X_train_scaled, y_train)\n",
        "# Mnb=model2.fit(X_train_scaled, y_train)\n",
        "# Bnb=model3.fit(X_train_scaled, y_train)\n",
        "\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "# #create a dictionary of our models\n",
        "# estimators=[('Complement', Cnb), ('Multinomial', Mnb), ('Bernoulli', Bnb)]\n",
        "# #create our voting classifier, inputting our models\n",
        "# ensemble = VotingClassifier(estimators, voting='hard')\n",
        "# submission=ensemble.fit(X_train_scaled, y_train)\n",
        "# ensemble.score(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20eec44",
      "metadata": {
        "id": "d20eec44"
      },
      "outputs": [],
      "source": [
        "submission=model.fit(X_train_scaled, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ac767d",
      "metadata": {
        "id": "60ac767d"
      },
      "outputs": [],
      "source": [
        "df_test=pd.read_csv('test.csv')\n",
        "## REMOVE PUNCTUATION\n",
        "df_test['text']=df_test['text'].apply(remove_punctuation)\n",
        "## TOKENISATION\n",
        "df_test.text=df_test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
        "## LEMMATIZATION\n",
        "submit=df_test['text']\n",
        "submit=submit.tolist()\n",
        "for i in range(len(submit)):\n",
        "    submit[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in submit[i]]\n",
        "## REMOVE STOPWORDS\n",
        "submit=pd.Series(submit)\n",
        "df_test['string_text']= submit.str.join(\" \")\n",
        "df_test['cleaned_text'] = df_test['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "submit=df_test['cleaned_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dae461b",
      "metadata": {
        "id": "4dae461b"
      },
      "outputs": [],
      "source": [
        "n=len(submit)\n",
        "corpus_submit=submit.tolist()\n",
        "submit = vectorizer.transform(corpus_submit)\n",
        "#submit_scaled = min_max_scaler.fit_transform(submit)\n",
        "submit=submission.predict(submit)\n",
        "kaggle=pd.DataFrame(columns=['id','label'])\n",
        "k=np.arange(n)+1\n",
        "kaggle['id']=pd.Series(k)\n",
        "kaggle['label']=pd.Series(submit)\n",
        "len(kaggle['id'])\n",
        "kaggle.to_csv('submission_corrected.csv',index=False)\n",
        "\n",
        "## WITHOUT SCALING - 0.73400, IMPROVES BY 0.1\n",
        "## WITH SCALING, DOWN BY 0.01 FOR MAX ABS \n",
        "##               DOWN BY 0.12 FOR STANDARD SCALER  \n",
        "## WITHOUT STOPWORDS - 0.654\n",
        "## USING COUNT_VECTORIZER - 0.674\n",
        "## with full dataset as train - 0.728 and score - 0.93\n",
        "## BY REPETITION, 0.74200\n",
        "## Spell checker and contractions brings down score\n",
        "## CURRENT - FULL SET AND ADDED PUNCTUATIONS - increase by 0.004"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c0a79e",
      "metadata": {
        "id": "d8c0a79e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb6b2da",
      "metadata": {
        "id": "edb6b2da"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a510e7b",
      "metadata": {
        "id": "7a510e7b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8203644",
      "metadata": {
        "id": "b8203644"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea42299",
      "metadata": {
        "id": "8ea42299"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9024249a",
      "metadata": {
        "id": "9024249a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1ef77c",
      "metadata": {
        "id": "3a1ef77c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e78bf0",
      "metadata": {
        "id": "74e78bf0"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([[1., 2., 3.],\n",
        "                 [4., 5., 6.]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "x @ tf.transpose(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a506c073",
      "metadata": {
        "id": "a506c073"
      },
      "outputs": [],
      "source": [
        "tdf=pd.read_csv('play_data.csv')\n",
        "y=tdf.label.values\n",
        "x=np.column_stack((tdf.YearsExperience.values,tdf.Salary.values))\n",
        "np.random.shuffle(x)  \n",
        "#type(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42e03a9",
      "metadata": {
        "id": "c42e03a9"
      },
      "outputs": [],
      "source": [
        "#np.random.choice()\n",
        "model=keras.Sequential([keras.layers.Dense(16,input_shape=(2,),activation='relu'),keras.layers.Dense(2,activation='relu')])\n",
        "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "model.fit(x,y,batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0497ca7b",
      "metadata": {
        "id": "0497ca7b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6e1233",
      "metadata": {
        "id": "6b6e1233"
      },
      "outputs": [],
      "source": [
        "x=X_train.toarray()\n",
        "y=y_train.values\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f220c38",
      "metadata": {
        "id": "4f220c38"
      },
      "outputs": [],
      "source": [
        "model=keras.Sequential([keras.layers.Dense(16,input_shape=(12107,),activation='relu'),keras.layers.Dense(6,activation='relu')])\n",
        "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "y=new_df.label.values\n",
        "model.fit(x[0:1000],y[0:1000],batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b3d655",
      "metadata": {
        "id": "32b3d655"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81220d0",
      "metadata": {
        "id": "e81220d0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de5e536",
      "metadata": {
        "id": "4de5e536"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7425b611",
      "metadata": {
        "id": "7425b611"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bbed85c",
      "metadata": {
        "id": "7bbed85c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6cdd39",
      "metadata": {
        "id": "5a6cdd39"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b2674b",
      "metadata": {
        "id": "e2b2674b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d883048",
      "metadata": {
        "id": "8d883048"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c209e5",
      "metadata": {
        "id": "d5c209e5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0740122",
      "metadata": {
        "id": "b0740122"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76796f13",
      "metadata": {
        "id": "76796f13"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abc7b98",
      "metadata": {
        "id": "5abc7b98"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a0c4f7",
      "metadata": {
        "id": "31a0c4f7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a9e6e7",
      "metadata": {
        "id": "28a9e6e7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57325a5c",
      "metadata": {
        "id": "57325a5c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4010e4aa",
      "metadata": {
        "id": "4010e4aa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c641895",
      "metadata": {
        "id": "9c641895"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6046b547",
      "metadata": {
        "id": "6046b547"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}