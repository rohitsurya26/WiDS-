{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dae01c7a",
      "metadata": {
        "id": "dae01c7a"
      },
      "source": [
        "# Sentiment Analysis\n",
        "#### By Rohit Suryavanshi\n",
        "Work Timeline doc: https://docs.google.com/document/d/1IXqmf_iipkzNDwZqWczPdeCnnNbQvESU1ktOREng1D0/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b72136d2",
      "metadata": {
        "scrolled": true,
        "id": "b72136d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988118b2-dcdb-4909-dc59-ced0cf9a5b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import nltk\n",
        "nltk.download(\"popular\")\n",
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3edf589c",
      "metadata": {
        "id": "3edf589c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "c2f292c0-1cac-4fd8-c303-a178536703f0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5371500efe80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#url = 'https://raw.githubusercontent.com/rohitsurya26/WiDS-/main/training.csv?token=GHSAT0AAAAAABQZ6QUJPBNBJ26UXSIVGRJ6YPM6TUQ'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training.csv'"
          ]
        }
      ],
      "source": [
        "#url = 'https://raw.githubusercontent.com/rohitsurya26/WiDS-/main/training.csv?token=GHSAT0AAAAAABQZ6QUJPBNBJ26UXSIVGRJ6YPM6TUQ'\n",
        "df = pd.read_csv('training.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "635a5f7b",
      "metadata": {
        "id": "635a5f7b"
      },
      "source": [
        "### Labels:\n",
        "\n",
        "0.   sadness\n",
        "1.   joy\n",
        "2.   love\n",
        "3.   anger\n",
        "4.   fear\n",
        "5.   surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f0064c08",
      "metadata": {
        "id": "f0064c08"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0f8f41",
      "metadata": {
        "id": "2f0f8f41"
      },
      "source": [
        "# Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd88c3e6",
      "metadata": {
        "id": "dd88c3e6"
      },
      "source": [
        "## Remove Punctuations\n",
        "#### Ref: https://studymachinelearning.com/text-preprocessing-removal-of-punctuations/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "451df6ba",
      "metadata": {
        "id": "451df6ba"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "regular_punct = list(string.punctuation)\n",
        "def remove_punctuation(text):\n",
        "    punct_list = regular_punct\n",
        "    for punc in punct_list:\n",
        "        if punc in text:\n",
        "            text = text.replace(punc, ' ')\n",
        "    return text.strip()\n",
        "df['punc_text']=df['text'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b924eb9",
      "metadata": {
        "id": "5b924eb9"
      },
      "source": [
        "## Tokenising Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7a5f3fbf",
      "metadata": {
        "id": "7a5f3fbf"
      },
      "outputs": [],
      "source": [
        "#nltk.download('punkt')\n",
        "df['tokenised']=df.apply(lambda row: nltk.word_tokenize(row['punc_text']), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e742c0fa",
      "metadata": {
        "id": "e742c0fa"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "065169bc",
      "metadata": {
        "id": "065169bc"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer=PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5fa1e5df",
      "metadata": {
        "id": "5fa1e5df"
      },
      "outputs": [],
      "source": [
        "text=df['tokenised']\n",
        "text=text.tolist()\n",
        "for i in range(len(text)):\n",
        "    text[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in text[i]]\n",
        "df['lemmatized']= pd.Series((i for i in text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55164ea8",
      "metadata": {
        "id": "55164ea8"
      },
      "source": [
        "## Removal of Stopwords\n",
        "##### Ref: https://stackabuse.com/removing-stop-words-from-strings-in-python/#usingpythonsnltklibraryÂ¶\n",
        "##### Ref: https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "864c2170",
      "metadata": {
        "id": "864c2170"
      },
      "outputs": [],
      "source": [
        "#nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "df['string_text']= df['lemmatized'].str.join(\" \")\n",
        "df['cleaned_text'] = df['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07e6fea",
      "metadata": {
        "id": "f07e6fea"
      },
      "source": [
        "### New Dataset with Pre-Processed Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "fa66d5dc",
      "metadata": {
        "id": "fa66d5dc"
      },
      "outputs": [],
      "source": [
        "new_df = pd.DataFrame(df['cleaned_text'])\n",
        "new_df['label']=df['label']\n",
        "new_df.to_csv('pre-processed_data.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "TvXGxSlMHhKM"
      },
      "id": "TvXGxSlMHhKM"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7bab6fdc",
      "metadata": {
        "id": "7bab6fdc"
      },
      "outputs": [],
      "source": [
        "## Splitting into training and test(C.V.) data set\n",
        "n=len(new_df['label'])\n",
        "idx_train = np.random.choice(n,int(n),replace=False) ## Gives us a list\n",
        "#idx_test = np.random.choice(n,int(n*0.2),replace=False) ## int bcuz n*0.2 is float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "61c83cca",
      "metadata": {
        "id": "61c83cca"
      },
      "outputs": [],
      "source": [
        "df_train=new_df.loc[idx_train]\n",
        "x_train=df_train['cleaned_text']\n",
        "y_train=df_train['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b77f57b5",
      "metadata": {
        "id": "b77f57b5"
      },
      "outputs": [],
      "source": [
        "# df_train,df_test=new_df.loc[idx_train],new_df.loc[idx_test]\n",
        "# x_train,x_test=df_train['cleaned_text'],df_test['cleaned_text']\n",
        "# y_train,y_test=df_train['label'],df_test['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "53e4ac3a",
      "metadata": {
        "id": "53e4ac3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus_train=x_train.tolist()\n",
        "#corpus_test=x_test.tolist()\n",
        "\n",
        "vectorizer =  CountVectorizer() \n",
        "#vectorizer =  TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(corpus_train)\n",
        "#X_test = vectorizer.transform(corpus_test)\n",
        "#type(X_train)\n",
        "vocab=vectorizer.get_feature_names_out()\n",
        "vocab_size=len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d9905f",
      "metadata": {
        "id": "90d9905f"
      },
      "source": [
        "# Deep Learning\n",
        "### Used Tensorflow and Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e95ae678",
      "metadata": {
        "id": "e95ae678"
      },
      "outputs": [],
      "source": [
        "import tensorflow \n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "9c92dc06",
      "metadata": {
        "id": "9c92dc06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b14ff0bb-3b5f-49fc-c83b-f8d1f4847f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zoom difficulties feel like give everything feel helpless alone desert cast ways voice action others another story zoom also temporarily loose view full picture\n"
          ]
        }
      ],
      "source": [
        "## Max length(words) in a sentence\n",
        "print(max(corpus_train))\n",
        "max_length=24 ## counted by hand"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de91e7e2",
      "metadata": {
        "id": "de91e7e2"
      },
      "source": [
        "### Used One-hot Encoding and Padding to input it into Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a6bede",
      "metadata": {
        "id": "48a6bede"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "encoded=[one_hot(i,vocab_size) for i in corpus_train]\n",
        "padded=pad_sequences(encoded,maxlen=max_length,padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7ed1f2",
      "metadata": {
        "id": "6d7ed1f2"
      },
      "outputs": [],
      "source": [
        "x=padded\n",
        "y=y_train.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de45ff2",
      "metadata": {
        "id": "4de45ff2"
      },
      "source": [
        "### Developing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "91ef83a2",
      "metadata": {
        "id": "91ef83a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "cb63739c-cd5f-4e57-8007-3f9da3107272"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f0a08efb6c41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0membeded_vector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeded_vector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#model.add(Flatten())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "\n",
        "embeded_vector_size=50\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embeded_vector_size, input_length=max_length,name=\"embedding\"))\n",
        "#model.add(Flatten())\n",
        "model.add(Conv1D())\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dense(units=16,activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b2b0cb2",
      "metadata": {
        "id": "3b2b0cb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "688c6edf-3ba9-4005-9667-ca204a1ddc1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 24, 50)            605350    \n",
            "                                                                 \n",
            " global_average_pooling1d (G  (None, 50)               0         \n",
            " lobalAveragePooling1D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                816       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 606,268\n",
            "Trainable params: 606,268\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2e3e54",
      "metadata": {
        "id": "5c2e3e54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c8bae9-99b3-4f25-c46c-3f53e57fe9e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "16000/16000 [==============================] - 77s 5ms/step - loss: 1.0206 - accuracy: 0.6311\n",
            "Epoch 2/2\n",
            "16000/16000 [==============================] - 78s 5ms/step - loss: 0.4450 - accuracy: 0.8473\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f66fd7f5410>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model.fit(x, y, epochs=2, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04d666a",
      "metadata": {
        "id": "a04d666a"
      },
      "outputs": [],
      "source": [
        "loss,accuracy=model.evaluate(x,y)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acd963f0",
      "metadata": {
        "id": "acd963f0"
      },
      "outputs": [],
      "source": [
        "url='https://raw.githubusercontent.com/rohitsurya26/WiDS-/main/test.csv?token=GHSAT0AAAAAABQZ6QUJJ4BQCEAY3FGG2FSAYPK2AOA'\n",
        "df_test=pd.read_csv(url)\n",
        "## REMOVE PUNCTUATION\n",
        "df_test['text']=df_test['text'].apply(remove_punctuation)\n",
        "## TOKENISATION\n",
        "df_test.text=df_test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
        "## LEMMATIZATION\n",
        "submit=df_test['text']\n",
        "submit=submit.tolist()\n",
        "for i in range(len(submit)):\n",
        "    submit[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in submit[i]]\n",
        "## REMOVE STOPWORDS\n",
        "submit=pd.Series(submit)\n",
        "df_test['string_text']= submit.str.join(\" \")\n",
        "df_test['cleaned_text'] = df_test['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "submit=df_test['cleaned_text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbd37047",
      "metadata": {
        "id": "dbd37047"
      },
      "outputs": [],
      "source": [
        "corpus_submit=submit.tolist()\n",
        " \n",
        "vectorizer =  CountVectorizer() \n",
        "\n",
        "submit_hmm = vectorizer.fit_transform(corpus_submit)\n",
        "#X_test = vectorizer.transform(corpus_test)\n",
        "#type(X_train)\n",
        "vocab=vectorizer.get_feature_names_out(submit_hmm)\n",
        "vocab_size=len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3c5f84",
      "metadata": {
        "id": "9a3c5f84"
      },
      "outputs": [],
      "source": [
        "#print(max(corpus_submit))\n",
        "max_length=24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e5acbfb",
      "metadata": {
        "id": "7e5acbfb"
      },
      "outputs": [],
      "source": [
        "encoded_submit=[one_hot(i,vocab_size) for i in corpus_submit]\n",
        "submit=pad_sequences(encoded_submit,maxlen=max_length,padding='post')\n",
        "#submit=encoded_submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd7c180",
      "metadata": {
        "id": "0dd7c180"
      },
      "outputs": [],
      "source": [
        "p=model.predict(submit)\n",
        "#print(p[:7])\n",
        "# p[np.where(a==np.max(p))] = 1\n",
        "b = np.zeros_like(p)\n",
        "b[np.arange(len(p)), p.argmax(1)] = 1\n",
        "kaggle=b\n",
        "# kaggle=np.round(p)\n",
        "# kaggle = p\n",
        "#print(kaggle[0:7])\n",
        "def labelling(matrix):\n",
        "    n=len(matrix)\n",
        "    lst=[1]*n\n",
        "    for i in range(n):\n",
        "        arr=matrix[i]\n",
        "        arr=list(arr)\n",
        "        idx=arr.index(max(arr))\n",
        "        lst[i]=idx        \n",
        "    return lst\n",
        "a=labelling(kaggle)        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b138f27",
      "metadata": {
        "id": "5b138f27"
      },
      "outputs": [],
      "source": [
        "kaggle=pd.DataFrame(columns=['id','label'])\n",
        "n=len(a)\n",
        "k=np.arange(n)+1\n",
        "kaggle['id']=pd.Series(k)\n",
        "kaggle['label']=pd.Series(a)\n",
        "len(kaggle['id'])\n",
        "kaggle.to_csv('submission_tensor.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4426cdd4",
      "metadata": {
        "id": "4426cdd4"
      },
      "source": [
        "# Naive Bayes Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35bd482a",
      "metadata": {
        "id": "35bd482a"
      },
      "source": [
        "## Basic Way to Approach\n",
        "1. Splitting dataframe in 80-20 way for training and test(C.V.) datasets\n",
        "2. Import TF-IDF vectorizer and corpus from x_train and x_test\n",
        "3. Apply vectorizer on corpus and obtain array containing TF-IDF values for each word\n",
        "4. Apply Complement Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d3413a3",
      "metadata": {
        "id": "8d3413a3"
      },
      "source": [
        "#### Implemented Scaling - Decreased Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bf6bdf",
      "metadata": {
        "id": "34bf6bdf"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "# scaler1 = preprocessing.StandardScaler(with_mean=False).fit(X_train) ## in [-2,2]\n",
        "# scaler2 = preprocessing.StandardScaler(with_mean=False).fit(X_test) ## with_mean=False is used for sparse matrices\n",
        "# X_train_scaled = scaler1.transform(X_train)\n",
        "# X_test_scaled = scaler2.transform(X_test)\n",
        "\n",
        "# min_max_scaler = preprocessing.MinMaxScaler() # to fit array in [0,1] range\n",
        "\n",
        "# min_max_scaler = preprocessing.MaxAbsScaler()\n",
        "# # This estimator scales and translates each feature individually such that the maximal absolute value of each feature \n",
        "# # in the training set will be 1.0. \n",
        "# # It does not shift/center the data, and thus does not destroy any sparsity.\n",
        "# # This scaler can also be applied to sparse CSR or CSC matrices.\n",
        "\n",
        "# X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
        "# X_test_scaled = min_max_scaler.fit_transform(X_test)\n",
        "\n",
        "# X_train_scaled.array\n",
        "# X_test_scaled.toarray()\n",
        "# y_train.array\n",
        "# y_test.array\n",
        "\n",
        "X_train_scaled=X_train\n",
        "#X_test_scaled=X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b11c21",
      "metadata": {
        "id": "21b11c21"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB ## something new score - 0.7\n",
        "from sklearn.naive_bayes import CategoricalNB ##sparse error \n",
        "from sklearn.naive_bayes import GaussianNB ##sparse error\n",
        "from sklearn.naive_bayes import ComplementNB ## dimension mismatch - max_score - 0.933125\n",
        "from sklearn.naive_bayes import MultinomialNB ## dimension mismatch - 0.8\n",
        "\n",
        "model = ComplementNB() ## 0.7240"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79625339",
      "metadata": {
        "id": "79625339"
      },
      "source": [
        "### ENSEMBLE LEARNING\n",
        "##### Decreased Accuracy since Complement NB was way stronger than multi and Bernoulli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fc7f2d",
      "metadata": {
        "id": "51fc7f2d"
      },
      "outputs": [],
      "source": [
        "### ENSEMBLE LEARNING\n",
        "\n",
        "# model2 = MultinomialNB() ## 0.61\n",
        "# model3 = BernoulliNB() ## 0.61400\n",
        "# Cnb=model1.fit(X_train_scaled, y_train)\n",
        "# Mnb=model2.fit(X_train_scaled, y_train)\n",
        "# Bnb=model3.fit(X_train_scaled, y_train)\n",
        "\n",
        "# from sklearn.ensemble import VotingClassifier\n",
        "# #create a dictionary of our models\n",
        "# estimators=[('Complement', Cnb), ('Multinomial', Mnb), ('Bernoulli', Bnb)]\n",
        "# #create our voting classifier, inputting our models\n",
        "# ensemble = VotingClassifier(estimators, voting='hard')\n",
        "# submission=ensemble.fit(X_train_scaled, y_train)\n",
        "# ensemble.score(X_test_scaled, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20eec44",
      "metadata": {
        "id": "d20eec44"
      },
      "outputs": [],
      "source": [
        "submission=model.fit(X_train_scaled, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ac767d",
      "metadata": {
        "id": "60ac767d"
      },
      "outputs": [],
      "source": [
        "df_test=pd.read_csv('test.csv')\n",
        "## REMOVE PUNCTUATION\n",
        "df_test['text']=df_test['text'].apply(remove_punctuation)\n",
        "## TOKENISATION\n",
        "df_test.text=df_test.apply(lambda row: nltk.word_tokenize(row['text']), axis=1)\n",
        "## LEMMATIZATION\n",
        "submit=df_test['text']\n",
        "submit=submit.tolist()\n",
        "for i in range(len(submit)):\n",
        "    submit[i]=[lemmatizer.lemmatize(word=word,pos='v') for word in submit[i]]\n",
        "## REMOVE STOPWORDS\n",
        "submit=pd.Series(submit)\n",
        "df_test['string_text']= submit.str.join(\" \")\n",
        "df_test['cleaned_text'] = df_test['string_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "submit=df_test['cleaned_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dae461b",
      "metadata": {
        "id": "4dae461b"
      },
      "outputs": [],
      "source": [
        "n=len(submit)\n",
        "corpus_submit=submit.tolist()\n",
        "submit = vectorizer.transform(corpus_submit)\n",
        "#submit_scaled = min_max_scaler.fit_transform(submit)\n",
        "submit=submission.predict(submit)\n",
        "kaggle=pd.DataFrame(columns=['id','label'])\n",
        "k=np.arange(n)+1\n",
        "kaggle['id']=pd.Series(k)\n",
        "kaggle['label']=pd.Series(submit)\n",
        "len(kaggle['id'])\n",
        "kaggle.to_csv('submission_corrected.csv',index=False)\n",
        "\n",
        "## WITHOUT SCALING - 0.73400, IMPROVES BY 0.1\n",
        "## WITH SCALING, DOWN BY 0.01 FOR MAX ABS \n",
        "##               DOWN BY 0.12 FOR STANDARD SCALER  \n",
        "## WITHOUT STOPWORDS - 0.654\n",
        "## USING COUNT_VECTORIZER - 0.674\n",
        "## with full dataset as train - 0.728 and score - 0.93\n",
        "## BY REPETITION, 0.74200\n",
        "## Spell checker and contractions brings down score\n",
        "## CURRENT - FULL SET AND ADDED PUNCTUATIONS - increase by 0.004"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c0a79e",
      "metadata": {
        "id": "d8c0a79e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb6b2da",
      "metadata": {
        "id": "edb6b2da"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a510e7b",
      "metadata": {
        "id": "7a510e7b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8203644",
      "metadata": {
        "id": "b8203644"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea42299",
      "metadata": {
        "id": "8ea42299"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9024249a",
      "metadata": {
        "id": "9024249a"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a1ef77c",
      "metadata": {
        "id": "3a1ef77c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "#from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e78bf0",
      "metadata": {
        "id": "74e78bf0"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([[1., 2., 3.],\n",
        "                 [4., 5., 6.]])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "x @ tf.transpose(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a506c073",
      "metadata": {
        "id": "a506c073"
      },
      "outputs": [],
      "source": [
        "tdf=pd.read_csv('play_data.csv')\n",
        "y=tdf.label.values\n",
        "x=np.column_stack((tdf.YearsExperience.values,tdf.Salary.values))\n",
        "np.random.shuffle(x)  \n",
        "#type(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c42e03a9",
      "metadata": {
        "id": "c42e03a9"
      },
      "outputs": [],
      "source": [
        "#np.random.choice()\n",
        "model=keras.Sequential([keras.layers.Dense(16,input_shape=(2,),activation='relu'),keras.layers.Dense(2,activation='relu')])\n",
        "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "model.fit(x,y,batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0497ca7b",
      "metadata": {
        "id": "0497ca7b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6e1233",
      "metadata": {
        "id": "6b6e1233"
      },
      "outputs": [],
      "source": [
        "x=X_train.toarray()\n",
        "y=y_train.values\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f220c38",
      "metadata": {
        "id": "4f220c38"
      },
      "outputs": [],
      "source": [
        "model=keras.Sequential([keras.layers.Dense(16,input_shape=(12107,),activation='relu'),keras.layers.Dense(6,activation='relu')])\n",
        "model.compile(optimizer='adam',loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
        "y=new_df.label.values\n",
        "model.fit(x[0:1000],y[0:1000],batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b3d655",
      "metadata": {
        "id": "32b3d655"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81220d0",
      "metadata": {
        "id": "e81220d0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de5e536",
      "metadata": {
        "id": "4de5e536"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7425b611",
      "metadata": {
        "id": "7425b611"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bbed85c",
      "metadata": {
        "id": "7bbed85c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a6cdd39",
      "metadata": {
        "id": "5a6cdd39"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2b2674b",
      "metadata": {
        "id": "e2b2674b"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d883048",
      "metadata": {
        "id": "8d883048"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c209e5",
      "metadata": {
        "id": "d5c209e5"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0740122",
      "metadata": {
        "id": "b0740122"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76796f13",
      "metadata": {
        "id": "76796f13"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abc7b98",
      "metadata": {
        "id": "5abc7b98"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31a0c4f7",
      "metadata": {
        "id": "31a0c4f7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a9e6e7",
      "metadata": {
        "id": "28a9e6e7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57325a5c",
      "metadata": {
        "id": "57325a5c"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4010e4aa",
      "metadata": {
        "id": "4010e4aa"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c641895",
      "metadata": {
        "id": "9c641895"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6046b547",
      "metadata": {
        "id": "6046b547"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "f07e6fea",
        "de91e7e2",
        "8d3413a3",
        "79625339"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}